{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8ypRAaNdu1nsEhLLPguj1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrfrkkan13/AWS-Project/blob/master/FinancialDataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import schedule\n",
        "import time\n",
        "import http.client\n",
        "import json\n",
        "from confluent_kafka import Producer, KafkaException\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# HTTP bağlantısı\n",
        "conn = http.client.HTTPSConnection(\"api.collectapi.com\")\n",
        "headers = {\n",
        "    'content-type': \"application/json\",\n",
        "    'authorization': \"apikey 0AHKzfMWRiI5Oat0f07Ky5:0TlKtB0ADuKnD2qhrg5vIN\"\n",
        "}\n",
        "\n",
        "# Kafka yapılandırması\n",
        "conf = {\n",
        "    'bootstrap.servers': 'pkc-6ojv2.us-west4.gcp.confluent.cloud:9092',\n",
        "    'sasl.mechanism': 'PLAIN',\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.username': 'IBNSXUJCSW3KCKT6',\n",
        "    'sasl.password': 'X8zMhoEppTP15+1Abj0LVMT8gFQVQZucU7XmxL2cS8LjIZm7HbOh6hbW1Wd8m6OZ'\n",
        "}\n",
        "\n",
        "# Kafka Producer oluşturma\n",
        "producer = Producer(**conf)\n",
        "\n",
        "# MongoDB bağlantısı\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "mongo_client = MongoClient(mongodb_host)\n",
        "db = mongo_client['databse']  # Veritabanı adı\n",
        "collection = db['jsonVeri']  # Koleksiyon adı\n",
        "\n",
        "# Veri çekme ve Kafka'ya gönderme fonksiyonu\n",
        "def fetch_and_send_data():\n",
        "    conn.request(\"GET\", \"/economy/currencyToAll?int=10&base=USD\", headers=headers)\n",
        "    res = conn.getresponse()\n",
        "    status = res.status\n",
        "    data = res.read()\n",
        "\n",
        "    print(\"HTTP Status:\", status)\n",
        "    print(\"Raw data:\", data)\n",
        "\n",
        "    try:\n",
        "        json_data = json.loads(data.decode(\"utf-8\"))\n",
        "        print(\"JSON data:\", json_data)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"JSON decode error:\", e)\n",
        "        json_data = None\n",
        "\n",
        "    if json_data:\n",
        "        # Kafka'ya gönderme\n",
        "        producer.produce('topic_0', key='currency_data', value=json.dumps(json_data))\n",
        "        producer.flush()\n",
        "\n",
        "        # MongoDB'ye kaydetme\n",
        "        collection.insert_one(json_data)\n",
        "        print('Message saved to MongoDB:', json_data)\n",
        "\n",
        "# Zamanlanmış görev\n",
        "schedule.every(6).minutes.do(fetch_and_send_data)\n",
        "\n",
        "# Ana döngü\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "riquqO9q-PUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
        "import os\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import certifi\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark.sql.functions import col, avg, when, lit\n",
        "\n",
        "# MongoDB bağlantı bilgileri\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "\n",
        "# MongoDB Atlas bağlantısı (sizin veritabanınıza göre değiştirin)\n",
        "client = MongoClient(mongodb_host)\n",
        "db = client[\"databse\"]\n",
        "collection = db[\"jsonVeri\"]\n",
        "\n",
        "\n",
        "\n",
        "def process_change(change):\n",
        "\n",
        "\n",
        "  # MongoDB bağlantısı\n",
        "  mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "  client = MongoClient(mongodb_host)\n",
        "\n",
        "  # Veritabanı ve koleksiyon seç\n",
        "  db = client.databse\n",
        "  collection = db.jsonVeri\n",
        "\n",
        "  # MongoDB koleksiyonundaki tüm verileri okuyun\n",
        "  cursor = collection.find()\n",
        "\n",
        "  # Spark oturumunu başlat\n",
        "  spark = SparkSession.builder \\\n",
        "      .appName(\"MongoDBtoSpark\") \\\n",
        "      .getOrCreate()\n",
        "\n",
        "  # Tüm dökümanları bir araya getiren bir liste oluştur\n",
        "  all_data = []\n",
        "  for document in cursor:\n",
        "      if 'result' in document and 'data' in document['result']:\n",
        "          all_data.extend(document['result']['data'])\n",
        "\n",
        "  # Pandas DataFrame'e dönüştür ve ardından Spark DataFrame'e\n",
        "  pdf = pd.DataFrame(all_data)\n",
        "  df = spark.createDataFrame(pdf)\n",
        "\n",
        "  # Döviz koduna göre gruplama ve rate değerlerini toplama\n",
        "  codes = df.select(\"code\").distinct().collect()\n",
        "  columns = [\"code\", \"name\"]\n",
        "  data = []\n",
        "\n",
        "  for code in codes:\n",
        "      code_value = code[\"code\"]\n",
        "      df_filtered = df.filter(df.code == code_value).orderBy(\"rate\")\n",
        "      rates = df_filtered.select(\"rate\").collect()\n",
        "      row = [code_value, df_filtered.select(\"name\").first()[\"name\"]]\n",
        "      for rate in rates:\n",
        "          row.append(rate[\"rate\"])\n",
        "      data.append(row)\n",
        "\n",
        "  # Yeni sütun adları oluştur\n",
        "  for i in range(len(data[0]) - 2):\n",
        "      columns.append(f\"rate_{i+1}\")\n",
        "\n",
        "  # Yeni DataFrame'i oluştur\n",
        "  new_pdf = pd.DataFrame(data, columns=columns)\n",
        "  new_df = spark.createDataFrame(new_pdf)\n",
        "\n",
        "  # Ortalama rate değerini hesapla ve ekle\n",
        "  rate_columns = [col(c) for c in new_df.columns if c.startswith(\"rate_\")]\n",
        "  new_df = new_df.withColumn(\"average_rate\", sum(rate_columns) / len(rate_columns))\n",
        "\n",
        "  # Son gelen rate değerini ortalama ile karşılaştır ve yeni sütunu ekle\n",
        "  last_rate_col = new_df.columns[-2]  # En son eklenen rate sütunu\n",
        "  new_df = new_df.withColumn(\n",
        "      \"comparison\",\n",
        "      when(col(last_rate_col) > col(\"average_rate\"), \"yükselen trend\").otherwise(\"alçalan trend\")\n",
        "  )\n",
        "\n",
        "  # Sadece gerekli sütunları seç ve göster\n",
        "  result_df = new_df.select(\"code\", \"name\", last_rate_col, \"average_rate\", \"comparison\")\n",
        "  result_df.show(n=result_df.count(), truncate=False)\n",
        "  # Spark DataFrame'i Pandas DataFrame'ine dönüştürme\n",
        "  result_pdf = result_df.toPandas()\n",
        "\n",
        "  # Pandas DataFrame'i MongoDB veritabanına kaydetme\n",
        "  result_df_dict = result_pdf.to_dict(\"records\")\n",
        "\n",
        "  # MongoDB koleksiyonu\n",
        "  result_collection = db[\"resultCollection2\"]\n",
        "\n",
        "  # Verileri MongoDB'ye ekleme\n",
        "  result_collection.insert_many(result_df_dict)\n",
        "\n",
        "  # Sonuçları doğrulama\n",
        "  for doc in result_collection.find():\n",
        "      print(doc)\n",
        "\n",
        "  # Spark oturumunu kapat\n",
        "  spark.stop()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# MongoDB Change Stream dinleyici\n",
        "change_stream = collection.watch()\n",
        "\n",
        "print(\"Dinlemeye başlandı...\")\n",
        "for change in change_stream:\n",
        "    process_change(change)\n"
      ],
      "metadata": {
        "id": "cy2U7mQntcH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yeni Bölüm"
      ],
      "metadata": {
        "id": "dH37YupEjmAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer, Consumer, KafkaException\n",
        "from confluent_kafka import Producer, KafkaException\n",
        "import json\n",
        "import requests\n",
        "import http.client\n",
        "from pymongo import MongoClient\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conn = http.client.HTTPSConnection(\"api.collectapi.com\")\n",
        "\n",
        "headers = {\n",
        "    'content-type': \"application/json\",\n",
        "    'authorization': \"apikey 0AHKzfMWRiI5Oat0f07Ky5:0TlKtB0ADuKnD2qhrg5vIN\"\n",
        "    }\n",
        "\n",
        "conn.request(\"GET\", \"/economy/currencyToAll?int=10&base=USD\", headers=headers)\n",
        "\n",
        "\n",
        "res = conn.getresponse()\n",
        "status = res.status\n",
        "data = res.read()\n",
        "\n",
        "print(\"HTTP Status:\", status)\n",
        "print(\"Raw data:\", data)\n",
        "\n",
        "try:\n",
        "    json_data = json.loads(data.decode(\"utf-8\"))\n",
        "    print(\"JSON data:\", json_data)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"JSON decode error:\", e)\n",
        "    json_data = None\n",
        "\n",
        "#######\n",
        "if json_data:\n",
        "\n",
        "\n",
        "    conf = {\n",
        "        'bootstrap.servers': 'pkc-6ojv2.us-west4.gcp.confluent.cloud:9092',\n",
        "        'sasl.mechanism': 'PLAIN',\n",
        "        'security.protocol': 'SASL_SSL',\n",
        "        'sasl.username': 'IBNSXUJCSW3KCKT6',\n",
        "        'sasl.password': 'X8zMhoEppTP15+1Abj0LVMT8gFQVQZucU7XmxL2cS8LjIZm7HbOh6hbW1Wd8m6OZ'\n",
        "    }\n",
        "\n",
        "    # Producer oluşturma\n",
        "    producer = Producer(**conf)\n",
        "\n",
        "    def delivery_report(err, msg):\n",
        "        if err is not None:\n",
        "            print('Message delivery failed: {}'.format(err))\n",
        "        else:\n",
        "            print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Kafka'ya gönderme\n",
        "    producer.produce('topic_0', key='currency_data', value=json.dumps(json_data), callback=delivery_report)\n",
        "    producer.flush()\n",
        "\n",
        "\n",
        "    # Consumer oluşturma\n",
        "    conf.update({\n",
        "        'group.id': 'my_group',\n",
        "        'auto.offset.reset': 'earliest'\n",
        "    })\n",
        "\n",
        "\n",
        "    consumer = Consumer(**conf)\n",
        "    consumer.subscribe(['topic_0'])\n",
        "\n",
        "\n",
        "    # MongoDB bağlantısı\n",
        "    mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "    mongo_client = MongoClient(mongodb_host)\n",
        "    db = mongo_client['databse']  # Veritabanı adı\n",
        "    collection = db['jsonVeri']  # Koleksiyon adı\n",
        "\n",
        "\n",
        "\n",
        "    while True:\n",
        "        msg = consumer.poll(timeout=1.0)\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                print('End of partition reached {0}/{1}'.format(msg.topic(), msg.partition()))\n",
        "            elif msg.error():\n",
        "                raise KafkaException(msg.error())\n",
        "        else:\n",
        "\n",
        "\n",
        "            # Kafka'dan gelen mesajı MongoDB'ye kaydetme\n",
        "            json_data = msg.value().decode('utf-8')\n",
        "            document = json.loads(json_data)\n",
        "            collection.insert_one(document)\n",
        "            print('Message saved to MongoDB:', document)\n"
      ],
      "metadata": {
        "id": "q0NB3_YYjmwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import schedule\n",
        "import time\n",
        "import http.client\n",
        "import json\n",
        "from confluent_kafka import Producer, KafkaException\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# HTTP bağlantısı\n",
        "conn = http.client.HTTPSConnection(\"api.collectapi.com\")\n",
        "headers = {\n",
        "    'content-type': \"application/json\",\n",
        "    'authorization': \"apikey 0AHKzfMWRiI5Oat0f07Ky5:0TlKtB0ADuKnD2qhrg5vIN\"\n",
        "}\n",
        "\n",
        "# Kafka yapılandırması\n",
        "conf = {\n",
        "    'bootstrap.servers': 'pkc-6ojv2.us-west4.gcp.confluent.cloud:9092',\n",
        "    'sasl.mechanism': 'PLAIN',\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.username': 'IBNSXUJCSW3KCKT6',\n",
        "    'sasl.password': 'X8zMhoEppTP15+1Abj0LVMT8gFQVQZucU7XmxL2cS8LjIZm7HbOh6hbW1Wd8m6OZ'\n",
        "}\n",
        "\n",
        "# Kafka Producer oluşturma\n",
        "producer = Producer(**conf)\n",
        "\n",
        "# MongoDB bağlantısı\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "mongo_client = MongoClient(mongodb_host)\n",
        "db = mongo_client['databse']  # Veritabanı adı\n",
        "collection = db['jsonVeri']  # Koleksiyon adı\n",
        "\n",
        "# Veri çekme ve Kafka'ya gönderme fonksiyonu\n",
        "def fetch_and_send_data():\n",
        "    conn.request(\"GET\", \"/economy/currencyToAll?int=10&base=USD\", headers=headers)\n",
        "    res = conn.getresponse()\n",
        "    status = res.status\n",
        "    data = res.read()\n",
        "\n",
        "    print(\"HTTP Status:\", status)\n",
        "    print(\"Raw data:\", data)\n",
        "\n",
        "    try:\n",
        "        json_data = json.loads(data.decode(\"utf-8\"))\n",
        "        print(\"JSON data:\", json_data)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"JSON decode error:\", e)\n",
        "        json_data = None\n",
        "\n",
        "    if json_data:\n",
        "        # Kafka'ya gönderme\n",
        "        producer.produce('topic_0', key='currency_data', value=json.dumps(json_data))\n",
        "        producer.flush()\n",
        "\n",
        "        # MongoDB'ye kaydetme\n",
        "        collection.insert_one(json_data)\n",
        "        print('Message saved to MongoDB:', json_data)\n",
        "\n",
        "# Zamanlanmış görev\n",
        "schedule.every(6).minutes.do(fetch_and_send_data)\n",
        "\n",
        "# Ana döngü\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)\n"
      ],
      "metadata": {
        "id": "plx_9S5ckYAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "import certifi\n",
        "import pandas as pd\n",
        "\n",
        "# MongoDB bağlantı bilgileri\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "\n",
        "# MongoDB istemcisi oluştur\n",
        "client = MongoClient(mongodb_host, tlsCAFile=certifi.where())\n",
        "\n",
        "\n",
        "# Veritabanı ve koleksiyon seç\n",
        "db = client.databse\n",
        "collection = db.jsonVeri\n",
        "\n",
        "\n",
        "# MongoDB koleksiyonundaki tüm verileri okuyun\n",
        "cursor = collection.find()\n",
        "# Verileri ekrana yazdırın\n",
        "# for document in cursor:\n",
        "#     print(document['result']['data'])\n",
        "\n",
        "\n",
        "#####ANALİZ\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Spark oturumu oluştur\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Currency Trend Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "\n",
        "# Verileriniz\n",
        "\n",
        "from pymongo import MongoClient\n",
        "from pyspark.sql import SparkSession\n",
        "import certifi\n",
        "\n",
        "# MongoDB bağlantı bilgileri\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "\n",
        "# Spark oturumu oluştur\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Currency Trend Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# MongoDB istemcisi oluştur\n",
        "client = MongoClient(mongodb_host, tlsCAFile=certifi.where())\n",
        "\n",
        "# Veritabanı ve koleksiyon seç\n",
        "db = client.databse\n",
        "collection = db.jsonVeri\n",
        "\n",
        "# MongoDB koleksiyonundaki tüm verileri okuyun\n",
        "cursor = collection.find()\n",
        "\n",
        "# Verileri analiz et\n",
        "for document in cursor:\n",
        "    if 'result' in document and 'data' in document['result']:\n",
        "        print(document['result']['data'])\n",
        "        # DataFrame'e dönüştürmek için uygun bir sözlük yapısı oluşturun\n",
        "        data_dict = {\n",
        "            'code': [],\n",
        "            'name': [],\n",
        "            'rate': []\n",
        "        }\n",
        "        for item in document['result']['data']:\n",
        "            data_dict['code'].append(item['code'])\n",
        "            data_dict['name'].append(item['name'])\n",
        "            data_dict['rate'].append(item['rate'])\n",
        "        # Veri çerçevesini oluşturun\n",
        "        df = spark.createDataFrame(pd.DataFrame(data_dict))\n",
        "        # Veri çerçevesini göster\n",
        "        df.show()\n",
        "        # Veri noktaları sayısını al\n",
        "        num_points = df.count()\n",
        "        print(\"Number of data points:\", num_points)\n",
        "        # Döviz kurlarının ortalama değerini hesapla\n",
        "        average_rate = df.agg({\"rate\": \"avg\"}).collect()[0][0]\n",
        "        print(\"Average exchange rate:\", average_rate)\n",
        "        # Döviz kurlarının standart sapmasını hesapla\n",
        "        std_dev = df.agg({\"rate\": \"stddev\"}).collect()[0][0]\n",
        "        print(\"Standard deviation of exchange rate:\", std_dev)\n",
        "    else:\n",
        "        print(\"Belgenin beklenen yapıya sahip olmadığı tespit edildi.\")\n",
        "\n",
        "# Spark oturumunu kapat\n",
        "spark.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pwS1jjE9kbh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "import certifi\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Spark oturumu MongoDB bağlantısı ile başlat\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MongoDBtoSpark\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
        "    .config(\"spark.mongodb.input.uri\", \"mongodb+srv://acalikoglu52:lqtTGbQShsu4O7hk@cluster0.p1kuepm.mongodb.net/mydatabase.mycollection\") \\\n",
        "    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://acalikoglu52:lqtTGbQShsu4O7hk@cluster0.p1kuepm.mongodb.net/mydatabase.mycollection\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# MongoDB bağlantı bilgileri\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "\n",
        "# MongoDB istemcisi oluştur\n",
        "client = MongoClient(mongodb_host, tlsCAFile=certifi.where())\n",
        "\n",
        "# Veritabanı ve koleksiyon seç\n",
        "db = client.databse\n",
        "collection = db.jsonVeri\n",
        "\n",
        "# MongoDB koleksiyonundaki tüm verileri okuyun\n",
        "cursor = collection.find()\n",
        "\n",
        "# Verileri ekrana yazdırın ve analiz et\n",
        "data_frames = []\n",
        "for document in cursor:\n",
        "    if 'result' in document and 'data' in document['result']:\n",
        "        print(document['result']['data'])\n",
        "        # DataFrame'e dönüştürmek için uygun bir sözlük yapısı oluşturun\n",
        "        data_dict = {\n",
        "            'code': [],\n",
        "            'name': [],\n",
        "            'rate': []\n",
        "        }\n",
        "        for item in document['result']['data']:\n",
        "            data_dict['code'].append(item['code'])\n",
        "            data_dict['name'].append(item['name'])\n",
        "            data_dict['rate'].append(item['rate'])\n",
        "        # Veri çerçevesini oluşturun\n",
        "        df = spark.createDataFrame(pd.DataFrame(data_dict))\n",
        "        data_frames.append(df)\n",
        "        # Veri çerçevesini göster\n",
        "        df.show(df.count(), truncate=False)\n",
        "    else:\n",
        "        print(\"Belgenin beklenen yapıya sahip olmadığı tespit edildi.\")\n",
        "\n",
        "# Tüm DataFrame'leri birleştirin\n",
        "if data_frames:\n",
        "    combined_df = data_frames[0]\n",
        "    for df in data_frames[1:]:\n",
        "        combined_df = combined_df.union(df)\n",
        "\n",
        "    # Aynı code sahip olanların aritmetik ortalamasını hesaplayın\n",
        "    average_df = combined_df.groupBy(\"code\").agg(\n",
        "        avg(\"rate\").alias(\"average_rate\")\n",
        "    )\n",
        "\n",
        "    # Sonucu gösterin\n",
        "    average_df.show(average_df.count(), truncate=False)\n",
        "\n",
        "# Spark oturumunu kapat\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "ai7a7XgvkeO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# MongoDB bağlantısı\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "client = MongoClient(mongodb_host)\n",
        "\n",
        "# Veritabanı ve koleksiyon seç\n",
        "db = client.databse\n",
        "collection = db.jsonVeri\n",
        "\n",
        "# MongoDB koleksiyonundaki tüm verileri okuyun\n",
        "cursor = collection.find()\n",
        "\n",
        "# Spark oturumunu başlat\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MongoDBtoSpark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Tüm dökümanları bir araya getiren bir liste oluştur\n",
        "all_data = []\n",
        "for document in cursor:\n",
        "    if 'result' in document and 'data' in document['result']:\n",
        "        all_data.extend(document['result']['data'])\n",
        "\n",
        "# Pandas DataFrame'e dönüştür ve ardından Spark DataFrame'e\n",
        "pdf = pd.DataFrame(all_data)\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Döviz koduna göre ortalama değerleri hesapla\n",
        "average_rates = df.groupBy(\"code\").agg(avg(\"rate\").alias(\"average_rate\"))\n",
        "\n",
        "# Orijinal DataFrame'e ortalama değerleri join ile ekle\n",
        "df_with_avg = df.join(average_rates, \"code\")\n",
        "\n",
        "# Her satır için ortalamanın üstünde veya altında olup olmadığını belirten bir sütun ekle\n",
        "df_with_avg = df_with_avg.withColumn(\n",
        "    \"comparison\",\n",
        "    F.when(col(\"rate\") > col(\"average_rate\"), \"ortalamanın üstünde\").otherwise(\"ortalamanın altında\")\n",
        ")\n",
        "\n",
        "# DataFrame'in tamamını göster\n",
        "df_with_avg.show(n=df_with_avg.count(), truncate=False)\n",
        "\n",
        "# Spark oturumunu kapat\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "mPiqYI-QkhZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, when\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# MongoDB bağlantısı\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "client = MongoClient(mongodb_host)\n",
        "\n",
        "# Veritabanı ve koleksiyon seç\n",
        "db = client.databse\n",
        "collection = db.jsonVeri\n",
        "\n",
        "# MongoDB koleksiyonundaki tüm verileri okuyun\n",
        "cursor = collection.find()\n",
        "\n",
        "# Spark oturumunu başlat\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MongoDBtoSpark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Tüm dökümanları bir araya getiren bir liste oluştur\n",
        "all_data = []\n",
        "for document in cursor:\n",
        "    if 'result' in document and 'data' in document['result']:\n",
        "        all_data.extend(document['result']['data'])\n",
        "\n",
        "# Pandas DataFrame'e dönüştür ve ardından Spark DataFrame'e\n",
        "pdf = pd.DataFrame(all_data)\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Döviz koduna göre ortalama değerleri hesapla\n",
        "average_rates = df.groupBy(\"code\").agg(avg(\"rate\").alias(\"average_rate\"))\n",
        "\n",
        "# Orijinal DataFrame'e ortalama değerleri join ile ekle\n",
        "df_with_avg = df.join(average_rates, \"code\")\n",
        "\n",
        "# Her satır için ortalamanın üstünde veya altında olup olmadığını belirten bir sütun ekle\n",
        "df_with_avg = df_with_avg.withColumn(\n",
        "    \"comparison\",\n",
        "    when(col(\"rate\") > col(\"average_rate\"), \"ortalamanın üstünde\").otherwise(\"ortalamanın altında\")\n",
        ")\n",
        "\n",
        "# Döviz koduna göre gruplama ve her döviz kodu için tek bir satırda toplama\n",
        "result_df = df_with_avg.groupBy(\"code\", \"name\").agg(\n",
        "    F.collect_list(col(\"rate\")).alias(\"rates\"),\n",
        "    avg(\"rate\").alias(\"average_rate\"),\n",
        "    F.collect_list(col(\"comparison\")).alias(\"comparisons\")\n",
        ")\n",
        "\n",
        "# DataFrame'in tamamını göster\n",
        "result_df.show(n=result_df.count(), truncate=False)\n",
        "\n",
        "# Spark oturumunu kapat\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "8124Xue7kjgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, when, lit\n",
        "import pandas as pd\n",
        "\n",
        "# MongoDB bağlantısı\n",
        "mongodb_host = \"mongodb+srv://mrfrkkan234:fvadYMhdJbqHoYvT@cluster0.qwbicey.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "client = MongoClient(mongodb_host)\n",
        "\n",
        "# Veritabanı ve koleksiyon seç\n",
        "db = client.databse\n",
        "collection = db.jsonVeri\n",
        "\n",
        "# MongoDB koleksiyonundaki tüm verileri okuyun\n",
        "cursor = collection.find()\n",
        "\n",
        "# Spark oturumunu başlat\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MongoDBtoSpark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Tüm dökümanları bir araya getiren bir liste oluştur\n",
        "all_data = []\n",
        "for document in cursor:\n",
        "    if 'result' in document and 'data' in document['result']:\n",
        "        all_data.extend(document['result']['data'])\n",
        "\n",
        "# Pandas DataFrame'e dönüştür ve ardından Spark DataFrame'e\n",
        "pdf = pd.DataFrame(all_data)\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Döviz koduna göre gruplama ve rate değerlerini toplama\n",
        "codes = df.select(\"code\").distinct().collect()\n",
        "columns = [\"code\", \"name\"]\n",
        "data = []\n",
        "\n",
        "for code in codes:\n",
        "    code_value = code[\"code\"]\n",
        "    df_filtered = df.filter(df.code == code_value).orderBy(\"rate\")\n",
        "    rates = df_filtered.select(\"rate\").collect()\n",
        "    row = [code_value, df_filtered.select(\"name\").first()[\"name\"]]\n",
        "    for rate in rates:\n",
        "        row.append(rate[\"rate\"])\n",
        "    data.append(row)\n",
        "\n",
        "# Yeni sütun adları oluştur\n",
        "for i in range(len(data[0]) - 2):\n",
        "    columns.append(f\"rate_{i+1}\")\n",
        "\n",
        "# Yeni DataFrame'i oluştur\n",
        "new_pdf = pd.DataFrame(data, columns=columns)\n",
        "new_df = spark.createDataFrame(new_pdf)\n",
        "\n",
        "# Ortalama rate değerini hesapla ve ekle\n",
        "rate_columns = [col(c) for c in new_df.columns if c.startswith(\"rate_\")]\n",
        "new_df = new_df.withColumn(\"average_rate\", sum(rate_columns) / len(rate_columns))\n",
        "\n",
        "# Son gelen rate değerini ortalama ile karşılaştır ve yeni sütunu ekle\n",
        "last_rate_col = new_df.columns[-2]  # En son eklenen rate sütunu\n",
        "new_df = new_df.withColumn(\n",
        "    \"comparison\",\n",
        "    when(col(last_rate_col) > col(\"average_rate\"), \"ortalamanın üstünde\").otherwise(\"ortalamanın altında\")\n",
        ")\n",
        "\n",
        "# DataFrame'in tamamını göster\n",
        "new_df.show(n=new_df.count(), truncate=False)\n",
        "\n",
        "# Spark oturumunu kapat\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "qMtXYRg3kkgy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}